<section class="relative h-[100vh]">
  <div class="absolute top-0 w-full h-[100vh] z-10 pointer-events-none">
    <div class="pt-[14rem]"></div>
    <div class="grid grid-cols-12 gap-x-3 gap-7 p-4 w-full">
      <div class="col-span-5"></div>
      <div class="col-span-7">
        <figure>
          <img
            class="w-full h-auto"
            src="/assets/ac.jpg"
            alt="Hunter Camera Collection Process"
          />
          <figcaption class="text-[0.7rem] text-gray-400">
            Fig. 1: Processo di raccolta delle immagini dalle fototrappole
          </figcaption>
        </figure>
      </div>
    </div>
  </div>

  <div
    class="sticky mt-28 z-30 top-28 font-bold text-gray-200 mix-blend-difference"
  >
    <div class="grid grid-cols-12 gap-x-3 gap-7 p-4 w-full text-xs uppercase">
      <div>I. SECTION ANIMATED</div>
      <div></div>
      <div></div>
      <div></div>
      <div></div>

      <div class="uppercase col-span-3 text-start z-40">
        Computer vision, a pivotal domain within artificial intelligence,
        focuses on enabling machines to interpret and derive meaningful
        information from digital images, videos, and other visual inputs.
        Analogous to the human visual system, which learns to recognize and
        contextualize visual stimuli through years of experience, computer
        vision systems must acquire this capability in a significantly
        compressed timeframe. This is typically achieved through deep learning
        methodologies, with
        <span class="text-green-600">Convolutional Neural Networks</span>
        (CNNs) playing a central role due to their capacity to model spatial
        hierarchies and learn high-level features from raw pixel data. In recent
        years, computer vision has evolved from simple pattern recognition to
        more complex visual reasoning, supported by a rapidly growing ecosystem
        of algorithms and pre-trained models. Among the techniques applied in
        this project are Optical Character Recognition (OCR)â€”used to extract
        textual information embedded within images and
      </div>

      <div
        class="uppercase col-span-3 text-start text-white font-bold mix-blend-difference"
      >
        <span class="text-green-600">zero-shot image classification</span>,
        which enables classification of visual content without requiring prior
        task-specific annotated examples. The latter is made possible through
        the use of vision-language models such as CLIP, which align textual and
        visual modalities in a shared embedding space. These methodologies have
        been extensively employed in the construction and processing of the
        Aculei dataset, which comprises over 16,000 automatically captured
        wildlife photographs. Given the scale and unstructured nature of the
        data, traditional manual annotation is infeasible. Thus, leveraging
        computer vision is not merely advantageous but essential for extracting
        semantic insights, identifying species, and organizing the dataset in a
        meaningful and interactive manner. The resulting archive serves both
        scientific and artistic goals, functioning as a computational lens into
        the natural ecosystem of the Umbrian forests while offering a novel mode
        of visual storytelling.
      </div>

      <div></div>
      <div></div>
      <div></div>
      <div></div>
    </div>
  </div>
</section>
