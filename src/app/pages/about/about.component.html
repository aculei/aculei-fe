<div class="w-screen mt-48 font-mono_lite text-gray-200">
  <div class="flex justify-center items-center text-sm">
    <div class="w-1/3 grid grid-cols-1 gap-6 uppercase text-center">
      <div>
        <p class="text-[0.6rem] text-gray-400">PHOTOGRAPHER & DIRECTOR</p>
        <a href="https://tobiafaverio.com" target="_blank">TOBIA FAVERIO</a>
      </div>
      <div class="grid grid-cols-3">
        <div>
          <p class="text-[0.6rem] text-gray-400">FRONTEND DEVELOPMENT</p>
          <a href="https://youssefhanna.it" target="_blank">YOUSSEF HANNA</a>
        </div>
        <div>
          <p class="text-[0.6rem] text-gray-400">AI DEVELOPMENT</p>
          <a href="https://benzebra.github.io" target="_blank"
            >FILIPPO BRAJUCHA</a
          >
        </div>
        <div>
          <p class="text-[0.6rem] text-gray-400">
            BACKEND, AI, INFRASTRUCTURE DEVELOPMENT
          </p>
          <a href="https://micheledinelli.github.io" target="_blank"
            >MICHELE DINELLI</a
          >
        </div>
      </div>
      <div>
        <p class="text-[0.6rem] text-gray-400">ART & WEB DESIGN</p>
        <a
          href="https://www.linkedin.com/in/martina-bracchi-0544491b0/"
          target="_blank"
          >MARTINA BRACCHI</a
        >
      </div>
    </div>
  </div>
  <div class="mt-36 flex justify-center items-center text-sm">
    <div class="w-screen grid grid-cols-4 uppercase text-center">
      <div class="justify-self-start">I. INTRODUCTION</div>
      <div class="justify-self-start col-span-3 text-left text-sm/6">
        Computer vision, a pivotal domain within artificial intelligence,
        focuses on enabling machines to interpret and derive meaningful
        information from digital images, videos, and other visual inputs.
        Analogous to the human visual system, which learns to recognize and
        contextualize visual stimuli through years of experience, computer
        vision systems must acquire this capability in a significantly
        compressed timeframe. This is typically achieved through deep learning
        methodologies, with Convolutional Neural Networks (CNNs) playing a
        central role due to their capacity to model spatial hierarchies and
        learn high-level features from raw pixel data. In recent years, computer
        vision has evolved from simple pattern recognition to more complex
        visual reasoning, supported by a rapidly growing ecosystem of algorithms
        and pre-trained models. Among the techniques applied in this project are
        Optical Character Recognition (OCR)—used to extract textual information
        embedded within images—and zero-shot image classification, which enables
        classification of visual content without requiring prior task-specific
        annotated examples. The latter is made possible through the use of
        vision-language models such as CLIP, which align textual and visual
        modalities in a shared embedding space. These methodologies have been
        extensively employed in the construction and processing of the Aculei
        dataset, which comprises over 16,000 automatically captured wildlife
        photographs. Given the scale and unstructured nature of the data,
        traditional manual annotation is infeasible. Thus, leveraging computer
        vision is not merely advantageous but essential for extracting semantic
        insights, identifying species, and organizing the dataset in a
        meaningful and interactive manner. The resulting archive serves both
        scientific and artistic goals, functioning as a computational lens into
        the natural ecosystem of the Umbrian forests while offering a novel mode
        of visual storytelling.
      </div>
    </div>
  </div>
</div>
