<div class="about-page absolute top-0 left-0 w-full pt-4">
  <div class="grid grid-cols-12 gap-x-3 gap-y-6 w-full">
    <div class="col-span-5"></div>
    <div class="col-span-6 text-xs text-gray-200 font-mono_lite uppercase">
      ACULEI.XYZ IS AN INTERACTIVE ARCHIVE BASED ON 30.342 IMAGES (APR.25)
      RECORDED SINCE 2020 BY 7 HUNTERCAMS AROUND THE PICCO DELL'AQUILA VALLEY,
      UMBRIA ITALY. AN AI, TRAINED TO RESPECT THE AUTONOMOUS SHOOTING SYSTEM,
      CLASSIFIES AND DIVIDES THE SELECTED IMAGES ACCORDING TO HIS OWN
      INTER&shy;PRETATION. AS TODAY ITS ACCURACY IS 93%. KEEP THIS IN MIND AS
      YOU NAVIGATE THROUGH THE ARCHIVE AND LET THE TECHNOLOGY DRIVE YOU AROUND
      THE VALLEY SWARM.
    </div>
    <div></div>

    <div class="col-span-5"></div>
    <div class="col-span-6 text-xs text-gray-200 font-mono_lite uppercase">
      Computer vision is a field of artificial intelligence (AI) focused on
      enabling computers to interpret, understand, and derive meaningful
      information from digital images, videos, and other visual inputs. It
      teaches machines to "see" and understand visual data similarly to humans.
      In the last decade the so known Convolutional Neural Networks (CNNs) have
      revolutionized computer vision tasks, enabling remarkable advancements in
      image analysis and recognition. CNNs are deep learning models that
      automatically learn spatial hierarchies of features from visual inputs,
      making them highly effective for analyzing images and videos. They power
      the majority of state-of-the-art computer vision systems by enabling
      accurate object recognition. CNNs have become the backbone of modern
      computer vision applications, enabling machines to achieve high accuracy
      in tasks such as image classification, object detection, facial
      recognition, and video analysis.
    </div>
    <div></div>

    <div class="col-span-5"></div>
    <div class="col-span-6 text-xs text-gray-200 font-mono_lite uppercase">
      Aculei relies on a combination of computer vision techniques to analyze
      and classify images. The process begins with the extraction of image
      embeddings using a pre-trained CLIP model. These embeddings are then
      clustered using K-means clustering, a popular unsupervised learning
      algorithm. The K-means algorithm groups similar data points together,
      allowing us to identify distinct clusters within the image embeddings.
      Image embeddings are high-dimensional representations of images that
      capture their semantic content. They are generated by passing images
      through a (deep) neural network, which encodes the visual features into a
      compact vector space. Representation of such a space is not trivial, as
      for us humans it is difficult to interpret high-dimensional data. To
      visualize the clusters formed by the K-means algorithm, we apply
      t-distributed stochastic neighbor embedding (t-SNE), a dimensionality
      reduction technique. t-SNE maps high-dimensional data into a
      lower-dimensional space while preserving the local structure of the data.
      This allows us to visualize the clusters in a 2D space, making it easier
      to understand the relationships between different image embeddings. The
      resulting visualization is shown in
      <a href="#fig.1" class="text-green-400">Fig. 1</a>.
      <figure class="my-3" id="fig.1">
        <img
          src="assets/about/clusters.png"
          alt="clusters"
          class="object-cover w-full h-auto"
        />
        <figcaption class="mt-2 text-gray-400">
          Fig. 1: The x and y axes represent the results of dimensionality
          reduction using t-SNE, applied to image embeddings generated by a
          pre-trained CLIP model (excluding the head layer). The visualized
          clusters correspond to the output of a K-means clustering algorithm
          applied to the reduced embeddings.
        </figcaption>
      </figure>
    </div>
    <div></div>

    <div class="col-span-5"></div>
    <div class="col-span-6 text-xs text-gray-200 font-mono_lite uppercase">
      In order to determine the optimal number of clusters for K-means, we
      employ the elbow method. The elbow method involves plotting the
      within-cluster sum of squares (WCSS) against the number of clusters as
      shown in
      <a href="/about#fig.2" class="text-green-400">Fig. 2</a>. WCSS measures
      the compactness of the clusters, and as the number of clusters increases,
      WCSS tends to decrease. The "elbow" point in the plot indicates the
      optimal number of clusters, where adding more clusters does not
      significantly reduce WCSS. This point represents a balance between the
      number of clusters and the compactness of the clusters. The elbow method
      is a widely used heuristic for selecting the number of clusters in K-means
      clustering.
      <figure class="my-3" id="fig.2">
        <img
          src="assets/about/elbow-method.png"
          alt="elbow-method"
          class="object-cover w-full h-auto"
        />
        <figcaption class="mt-2 text-gray-400">
          Fig. 2: The x axis represent the number of clusters, while the y axis
          represents the inertia of the K-means clustering algorithm.
        </figcaption>
      </figure>
    </div>
    <div></div>

    <div class="col-span-5"></div>
    <div class="col-span-6 text-xs text-gray-200 font-mono_lite uppercase">
      The clustering process is useful to understand the distribution of image
      embeddings but does not provide a direct classification of the images. To
      achieve this, we employ a pre-trained CLIP model. CLIP (Contrastive
      Language-Image Pretraining) is a powerful model that learns to associate
      images and text by training on a large dataset of image-text pairs. It
      generates embeddings for both images and text, allowing us to compare
      their semantic similarity. By using CLIP, we can leverage its ability to
      understand the relationship between images and text to classify the images
      in our dataset. CLIP recognize the content of the images and generate
      embeddings that capture their semantic meaning. This enables us to
      classify the images based on their visual content and associate them with
      relevant textual descriptions. The classification process involves
      comparing the image embeddings with a set of predefined text labels or
      descriptions. The model calculates the similarity between the image
      embeddings and the text embeddings, allowing us to determine the most
      relevant label for each image. The classification results are then
      presented in the archive, where users can explore the images and their
      corresponding labels. The classification process produces probabilistic
      results that indicate the likelihood of each image belonging to a specific
      class. The probabilities are obtained by comparing the image embeddings
      with the text embeddings of the predefined labels. The model calculates
      the similarity scores between the image and text embeddings, and these
      scores are transformed into probabilities using a softmax function. It may
      happen that a certain image is misclassified, for this reason the archive
      shows the top 3 most probable classes for each image.
    </div>
    <div></div>
    <div class="col-span-5"></div>
    <div class="col-span-6 text-xs text-gray-200 font-mono_lite uppercase">
      A cool visualization of one of the numerous stepts that the CLIP model
      performs to classify the images is the SIFT algorithm. SIFT
      (Scale-Invariant Feature Transform) is a computer vision algorithm used to
      detect and describe local features in images. It identifies keypoints in
      an image and computes descriptors for these keypoints, which are invariant
      to scale, rotation, and affine transformations. SIFT is widely used in
      various applications, including image stitching, object recognition, and
      3D reconstruction. The algorithm works by detecting keypoints in the image
      using a difference of Gaussian (DoG) approach. It identifies points of
      interest that are stable across different scales and orientations. Once
      the keypoints are detected, SIFT computes descriptors for each keypoint,
      which are vectors that describe the local image region around the
      keypoint. In <a href="#fig.3" class="text-green-400">Fig.3</a> we show the
      comparison between two images. The SIFT algorithm detects keypoints in
      both images and matches them based on their descriptors. The lines
      connecting the keypoints indicate the matches found by the algorithm. This
      visualization illustrates how SIFT can identify corresponding features in
      different images, even when they are taken from different perspectives or
      under varying conditions.
      <figure class="my-3" id="fig.3">
        <img
          src="assets/about/sift.png"
          alt="clusters"
          class="object-cover w-full h-auto"
        />
        <figcaption class="mt-2 text-gray-400">
          Fig. 3: An example of the SIFT algorithm applied to two images.
          Keypoints are detected and matched between the two images. The lines
          connecting the keypoints indicate the matches found by the algorithm.
        </figcaption>
      </figure>
    </div>
    <div></div>
  </div>

  <div
    class="mt-6 mb-24 grid grid-cols-12 gap-x-3 p-4 w-full text-xs text-gray-200 font-mono_lite uppercase"
  >
    <div class="col-span-6"></div>
    <div class="col-span-2 flex flex-col gap-2">
      <p>PHOTOs & CONCEPT</p>
      <p>DEVELOPMENT</p>
      <p class="">DESIGN</p>
    </div>
    <div></div>
    <div class="col-span-2 flex flex-col gap-2">
      <p>
        <a href="https://tobiafaverio.com" target="_blank">TOBIA FAVERIO</a>
      </p>
      <p>
        <a href="https://benzebra.github.io" target="_blank">FILIPPO BRAJUCHA</a
        >,
        <a href="https://micheledinelli.github.io" target="_blank"
          >MICHELE DINELLI</a
        >,
        <a href="https://youssefhanna.it" target="_blank">YOUSSEF HANNA</a>
      </p>
      <p>
        <a href="https://www.instagram.com/martina_bracchi_" target="_blank"
          >MARTINA BRACCHI</a
        >
      </p>
    </div>
    <div></div>

    <div class="col-span-9"></div>
    <div class="col-span-3">
      ALL IMAGES SHOT &#64;
      <a
        href="https://www.lafonte-agriturismo.it/"
        target="_blank"
        class="underline"
        >LA FONTE</a
      >, PERUGIA, ITALY
    </div>
  </div>
</div>
